{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PRO_C126_PP_V1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Tiro al blanco** es un juego donde el jugador dispara una flecha puntiaguda hacia un objetivo circular con 10 anillos.\n",
        "\n",
        "<img src=\"https://s3-whjr-curriculum-uploads.whjr.online/4de9132a-c71d-42ce-9099-3293e8805fd9.jpg\"> "
      ],
      "metadata": {
        "id": "nUWO5QkC_g-4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problema de aprendizaje por refuerzo (RL) a solucionar\n",
        "Golpea el centro del objetivo con la recompensa máxima."
      ],
      "metadata": {
        "id": "5QtHLAqv3wP3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://s3-whjr-curriculum-uploads.whjr.online/40656a8c-14e2-4dd7-9f9e-4c17669b9182.png\" width=300>\n",
        "\n",
        "\n",
        "Número de **estado**: ?\n",
        "\n",
        "Número de **acciones**: ?\n",
        "\n"
      ],
      "metadata": {
        "id": "Osb6FQ74YZtE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importar las bibliotecas"
      ],
      "metadata": {
        "id": "M2oIipDmeqap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Matriz de recompensas\n",
        "La matriz de recompensas representa los estados como filas y las acciones como columnas con los valores de recompensa respectivos asignados a un par estado-acción dado."
      ],
      "metadata": {
        "id": "Ujmi3BO54LfJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear la matriz de recompensas"
      ],
      "metadata": {
        "id": "OUqPgOl0eh2u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Realizar una acción aleatoriamente"
      ],
      "metadata": {
        "id": "Af-CAmdfkDQY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definir shoot()"
      ],
      "metadata": {
        "id": "ibSLCyMyigmK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Matriz-Q\n",
        "\n",
        "**Q-learning** es un algoritmo de aprendizaje por refuerzo (RA). Dado el estado actual, ayuda a encontrar la mejor acción a realizar por el agente.\n",
        "\n",
        "La **matriz-Q** representa la recompensa recibida después de tomar una acción particular en el estado actual. Inicialmente, todos los elementos de la matriz-Q son ceros.\n"
      ],
      "metadata": {
        "id": "JXKyVT28hHoH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear la matriz-Q"
      ],
      "metadata": {
        "id": "aNYwOV7ogtw1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Realizar una acción"
      ],
      "metadata": {
        "id": "0c95A4SOkGdN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Definir take_action()\n",
        "\n",
        "def take_action(reward_matrix):\n",
        "  \n",
        "     # Llamar a la función shoot() para obtener la acción\n",
        "\n",
        "     # Imprimir la acción \n",
        "\n",
        "     # Obtener la recompensa correspondiente usando la matriz de recompensas\n",
        "\n",
        "     # Imprimir la recompensa\n",
        "\n",
        "     return action, reward"
      ],
      "metadata": {
        "id": "LSBm-8CJ0UfK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Actualizar matriz-Q"
      ],
      "metadata": {
        "id": "APyameZoUbdA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definir el método run_episode()\n",
        "\n",
        "def run_episode(reward_matrix, shoot_per_game=5):\n",
        "\n",
        "  score = 0\n",
        "\n",
        "  # Usar un bucle 'For' para repetir el número de oportunidades\n",
        "\n",
        "        # Imprimir el número de tiro\n",
        "\n",
        "        # Llamar al método take_action para obtener la acción\n",
        "\n",
        "        # Aumentar la puntuación\n",
        "\n",
        "        # Imprimir el número de tiros final\n",
        "\n",
        "  # Actualizar la matriz-Q\n",
        "\n",
        "  # Regresar la matriz-Q actualizada\n"
      ],
      "metadata": {
        "id": "AmMTU4D-Ub8q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Llamar al método run_episode para verificar la matriz-Q final para un episodio\n"
      ],
      "metadata": {
        "id": "U1D914ElVHhZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Entrenamiento\n",
        "\n",
        "Crea una función que muestre el número de juegos, se ejecute un número dado de veces y calcule la recompensa promedio para cada uno."
      ],
      "metadata": {
        "id": "xsvq86xTVRvE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definir la función train()\n",
        "\n",
        "def train(episodes):\n",
        "  # Usar un bucle 'For' para repetir a través de los episodios\n",
        "\n",
        "    # Inicializar la variable total_reward\n",
        "\n",
        "    # Imprimir 'episodio inicial' con el número de episodio\n",
        "\n",
        "    # Llamar al método run_episode() para obtener la matriz-Q para un episodio\n",
        "    \n",
        "    # La recompensa del episodio será la suma de todas las recompensas para un episodio\n",
        "    \n",
        "    # Imprimir la recompensa del episodio\n",
        "    \n",
        "    # La recompensa total será la suma de todas las recompesas de los episodios\n",
        "    \n",
        "\n",
        "  # Regresar total_reward\n",
        "  "
      ],
      "metadata": {
        "id": "1UtIAfvCVRen",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "outputId": "08fcecd9-b54f-491b-fc19-66a6664e7d05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-8-376fb43b13df>\"\u001b[0;36m, line \u001b[0;32m20\u001b[0m\n\u001b[0;31m    \u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Entrena 1000 episodios"
      ],
      "metadata": {
        "id": "hb9uC2bQVvnF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ejecutar la función train() para 1000 episodios\n",
        "\n",
        "\n",
        "# Imprimir la recompensa promedio\n"
      ],
      "metadata": {
        "id": "dobDyUmpV1zY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusión: \n",
        "\n",
        "Esto nos da una buena idea sobre el desempeño general del aprendizaje por refuerzo más sencillo con el problema de **un estado con varias acciones**, también conocido como el problema del \"**bandido con K brazos**\".\n",
        "\n",
        "Uno de los usos más importantes de este tipo de problemas puede ser apreciado al seleccionar el anuncio correcto de los varios que pueden mostrarse en una página web. ¡La computadora puede aprender a elegir el mejor anuncio con el mayor número de clics de usuarios!"
      ],
      "metadata": {
        "id": "YvnM6DSAWKiy"
      }
    }
  ]
}